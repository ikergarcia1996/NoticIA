#Training args
model_name_or_path: upstage/SOLAR-10.7B-Instruct-v1.0
torch_dtype: bfloat16
trust_remote_code: true
use_lora: false
quantization_inference: null
gradient_checkpointing: true
force_auto_device_map: false
use_flash_attention: true
deepspeed: configs/deepspeed_configs/deepspeed_zero3.json
generation_config: generation_config.json
stop_words:
  - "<s>"
  - "</s>"
  - "\\n"
  - "### User:"
  - "### Assistant:"
  - "###"

# dataset arguments
train_datasets: 
  - train
validation_datasets: 
  - validation
test_datasets: 
  - test

max_seq_length: 8192
generation_max_length: 8192

# checkpoint settings
output_dir: results/zero/upstage_SOLAR-10.7B-Instruct-v1.0
overwrite_output_dir: true

# evaluation
do_train: false
do_eval: false
do_predict: true
predict_with_generate: true

# batch size: 16 batch size * 8 gradaccum * 1 GPUs = 128
per_device_train_batch_size: 4
per_device_eval_batch_size: 2
gradient_accumulation_steps: 2
generation_num_beams: 1


# reporting
logging_strategy: steps
logging_first_step: true
logging_steps: 5
report_to: wandb
run_name: "SOLAR-10.7B-Instruct-v1.0"
disable_tqdm: false


# performance
bf16: true
fp16: false
torch_compile: false
ddp_find_unused_parameters: false
